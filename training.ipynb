{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.transforms as T\n",
    "from presnet import PResNet\n",
    "from hybrid_encoder import HybridEncoder\n",
    "from rtdetrv2_decoder import RTDETRTransformerv2\n",
    "from rtdetr_postprocessor import RTDETRPostProcessor\n",
    "from matcher import HungarianMatcher\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "from ema import ModelEMA\n",
    "from warmup import LinearWarmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Feature Extractor\n",
    "presnet = PResNet(depth= 18,\n",
    "                  variant = 'd',\n",
    "                  freeze_at = -1,\n",
    "                  return_idx = [1, 2, 3],\n",
    "                  num_stages = 4,\n",
    "                  freeze_norm = False,\n",
    "                  pretrained = True, \n",
    "                  act=\"leaky_relu\",)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = HybridEncoder(  in_channels = [128, 256, 512],\n",
    "                          feat_strides = [8, 16, 32],\n",
    "                          # intra\n",
    "                          hidden_dim = 256,\n",
    "                          use_encoder_idx = [2],\n",
    "                          num_encoder_layers = 2,\n",
    "                          nhead = 8,\n",
    "                          dim_feedforward = 1024,\n",
    "                          dropout = 0.,\n",
    "                          enc_act = 'gelu' ,\n",
    "                          # cross\n",
    "                          expansion = 0.5,\n",
    "                          depth_mult = 1,\n",
    "                          act = 'silu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = RTDETRTransformerv2(feat_channels = [256, 256, 256],\n",
    "                              feat_strides = [8, 16, 32],\n",
    "                              hidden_dim = 256,\n",
    "                              num_levels = 3,\n",
    "                              num_layers = 3,\n",
    "                              num_queries = 300,\n",
    "                              num_denoising = 200,\n",
    "                              label_noise_ratio = 0.5,\n",
    "                              box_noise_scale = 1.0, # 1.0 0.4\n",
    "                              eval_idx = -1,\n",
    "                              # NEW\n",
    "                              num_points = [4, 4, 4], # [3,3,3] [2,2,2]\n",
    "                              cross_attn_method = 'default', # default, discrete\n",
    "                              query_select_method = 'agnostic', # default, agnostic \n",
    "                              num_classes=1,\n",
    "                              activation=\"relu\",\n",
    "                              dropout=0\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocessor = RTDETRPostProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, ) -> None:\n",
    "        super().__init__()\n",
    "        self.backbone = presnet\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, images, targets = None):\n",
    "        features = self.backbone(images)\n",
    "        features = self.encoder(features)\n",
    "        out = self.decoder(features, targets)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detr = Model()\n",
    "# Training Loop\n",
    "detr.train()\n",
    "detr.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "from dataset import PersonDataset\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "root_dir = '/media/enrique/Extreme SSD/person'\n",
    "sequence_list = [f'person-{i}' for i in range(1, 21)]\n",
    "sequence_list = [\"person-9\"]\n",
    "dataset = PersonDataset(root_dir=root_dir, sequence_list=sequence_list, img_transform_size=(640, 640), template_transform_size=(256, 256), max_num_templates=10, max_detections = 300)\n",
    "\n",
    "# Define the lengths for training and validation sets\n",
    "train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "val_size = len(dataset) - train_size  # The rest for validation\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "batch_size = 1\n",
    "# Optionally, create DataLoader objects for the training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Function to visualize a batch of images from the dataloader\n",
    "# def visualize_batch_from_dataloader(dataloader):\n",
    "#     # Get a batch of images from the dataloader\n",
    "#     for batch in dataloader:\n",
    "#         # Assuming 'img' is the key in the dataset that holds the images\n",
    "#         images = batch['img']  # Shape: [batch_size, channels, height, width]\n",
    "\n",
    "#         # Loop through the images in the batch\n",
    "#         for i in range(images.size(0)):\n",
    "#             # Convert the image tensor to NumPy and rearrange the dimensions\n",
    "#             image_np = images[i].permute(1, 2, 0).cpu().numpy()  # Convert to [height, width, channels]\n",
    "            \n",
    "#             # Assuming images are in the range [0, 1], scale to [0, 255]\n",
    "#             image_np = (image_np * 255).astype(np.uint8)\n",
    "\n",
    "#             # Display the image using OpenCV\n",
    "#             cv2.imshow(f\"Image {i}\", image_np)\n",
    "\n",
    "#             # Wait for a key press\n",
    "#             cv2.waitKey(0)\n",
    "\n",
    "#         # Close all OpenCV windows\n",
    "#         cv2.destroyAllWindows()\n",
    "#         break  # Only visualize the first batch\n",
    "\n",
    "# # Now use this function to visualize an image from the dataloader\n",
    "# visualize_batch_from_dataloader(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from rtdetrv2_criterion import RTDETRCriterionv2\n",
    "from torch.cuda.amp import autocast\n",
    "import torch.amp as amp\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "# Defining the Optimizer\n",
    "learning_rate = 0.0001\n",
    "backbone_learning_rate = 0.00001\n",
    "optimizer = optim.AdamW([{'params':detr.backbone.parameters(), 'lr': backbone_learning_rate},\n",
    "                         {'params':detr.encoder.parameters(), 'lr': learning_rate},\n",
    "                         {'params':detr.decoder.parameters(), 'lr': learning_rate}], lr=learning_rate, weight_decay=0.0001, betas=[0.9, 0.999])\n",
    "\n",
    "# Defining the criterion\n",
    "weight_dict={'cost_class': 2, 'cost_bbox':5, 'cost_giou':2}\n",
    "matcher = HungarianMatcher(use_focal_loss=False, weight_dict = weight_dict, alpha=0.25, gamma=2.0)\n",
    "losses = ['vfl', 'boxes']\n",
    "weight_dict = {'loss_vfl': 1, 'loss_bbox': 5, 'loss_giou': 2}\n",
    "criterion = RTDETRCriterionv2(losses=losses, weight_dict=weight_dict, matcher=matcher, num_classes=num_classes, alpha=0.75, gamma=2.0)\n",
    "criterion.train()\n",
    "\n",
    "ema = ModelEMA(detr)\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 30\n",
    "\n",
    "# Initialize the GradScaler\n",
    "scaler = amp.GradScaler()\n",
    "scaler = None\n",
    "\n",
    "# initialize max norm clip\n",
    "max_norm = 0.1\n",
    "\n",
    "# Create the Linear Warmup scheduler\n",
    "lr_scheduler = MultiStepLR(optimizer, [1000], gamma=0.1)\n",
    "\n",
    "lr_warmup_scheduler = LinearWarmup(lr_scheduler = lr_scheduler, warmup_duration = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "# Weights and biases config\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"UltimateModel\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"architecture\": \"RT-DETR\",\n",
    "    \"dataset\": \"Lasot_Person\",\n",
    "    \"epochs\": num_epochs,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your drawing function to draw bounding boxes and scores\n",
    "def draw_predictions(image, boxes, scores, score_threshold=0.001):\n",
    "    h, w = image.shape[:2]\n",
    "    \n",
    "    for i, score in enumerate(scores):\n",
    "        # Convert bbox format from (x, y, w, h) in unit scale to (x1, y1, x2, y2) in pixel scale\n",
    "        x, y, bw, bh = boxes[i]\n",
    "        x1 = int((x - bw / 2) * w)\n",
    "        y1 = int((y - bh / 2) * h)\n",
    "        x2 = int((x + bw / 2) * w)\n",
    "        y2 = int((y + bh / 2) * h)\n",
    "        \n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        # Draw the objectness score\n",
    "        label = f\"Objectness: {score[0]:.2f}\"\n",
    "        cv2.putText(image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths for saving models\n",
    "checkpoint_path = \"detr_checkpoint_desk.pth\"\n",
    "output_image_path = \"output_images\"\n",
    "N = 1000  # Save images every N iterations\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_image_path, exist_ok=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training Phase\n",
    "    detr.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    total_batches = len(dataloader)\n",
    "    total_samples = len(dataloader.dataset)\n",
    "    samples_processed = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        global_step = epoch * len(dataloader) + batch_idx\n",
    "        metas = dict(epoch=epoch, step=batch_idx, global_step=global_step)\n",
    "\n",
    "        images = batch['img']  # Input images\n",
    "        bounding_boxes_batch = batch['bounding_boxes']  # Target bounding boxes for the whole batch\n",
    "        num_boxes_batch = batch['num_boxes']  # Number of valid bounding boxes for the whole batch\n",
    "\n",
    "        batch_size = images.size(0)\n",
    "        samples_processed += batch_size\n",
    "\n",
    "        # Create a list of dictionaries for targets\n",
    "        targets = []\n",
    "        for i in range(len(images)):\n",
    "            labels = torch.full((num_boxes_batch[i],), 0, dtype=torch.long) \n",
    "            targets.append({'labels': labels, 'boxes': bounding_boxes_batch[i][:num_boxes_batch[i],:]})\n",
    "\n",
    "        for target in targets:\n",
    "            print(\"BATCH\")\n",
    "            print(\"LABELS\", target[\"labels\"])\n",
    "            print(\"BOXES\", target[\"boxes\"])\n",
    "\n",
    "        # Move data to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.cuda()\n",
    "            targets = [{'labels': target['labels'].cuda(), 'boxes': target['boxes'].cuda()} for target in targets]\n",
    "\n",
    "        if scaler is not None:\n",
    "            with torch.autocast(device_type=str(device_str), cache_enabled=True):\n",
    "                outputs = detr(images, targets=targets)\n",
    "            \n",
    "            with torch.autocast(device_type=str(device_str), enabled=False):\n",
    "                loss_dict = criterion(outputs, targets, **metas)\n",
    "\n",
    "            loss: torch.Tensor = sum(loss_dict.values()); loss_dict[\"total_loss\"] = loss\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if max_norm > 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(detr.parameters(), max_norm)\n",
    "\n",
    "            # Unscale the gradients and step the optimizer\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        else:\n",
    "            outputs = detr(images, targets=targets)\n",
    "\n",
    "            loss_dict = criterion(outputs, targets, **metas)\n",
    "\n",
    "            loss: torch.Tensor = sum(loss_dict.values()); loss_dict[\"total_loss\"] = loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            if max_norm > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(detr.parameters(), max_norm)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        if lr_warmup_scheduler is not None:\n",
    "            lr_warmup_scheduler.step()\n",
    "\n",
    "\n",
    "        if ema is not None:\n",
    "            ema.update(detr)\n",
    "\n",
    "        # print(\"outputs[pred_boxes]\", outputs[\"pred_boxes\"])\n",
    "\n",
    "        # Save output images every N iterations\n",
    "        if batch_idx % N == 0:\n",
    "            print(\"TIME TO DRAW SOME IMAGES\")\n",
    "            print(\"TIME TO DRAW SOME IMAGES\")\n",
    "            print(\"TIME TO DRAW SOME IMAGES\")\n",
    "\n",
    "            pred_logits = outputs[\"pred_logits\"]  # [batch, max_detections, num_classes]\n",
    "            pred_boxes = outputs[\"pred_boxes\"]  # [batch, max_detections, 4]\n",
    "\n",
    "            \n",
    "            # Iterate over the batch\n",
    "            for i in range(pred_logits.size(0)):  # Iterate through batch\n",
    "                image_np = images[i].permute(1, 2, 0).cpu().numpy()  # Convert image to numpy\n",
    "                image_np = (image_np * 255).astype(np.uint8)  # Scale to [0, 255]\n",
    "                \n",
    "                print(\"pred_logits[i]\", pred_logits[i].shape)\n",
    "\n",
    "                # Apply sigmoid to get objectness score\n",
    "                scores = torch.sigmoid(pred_logits[i])\n",
    "\n",
    "                topk_logits = torch.topk(scores.squeeze(), 5)\n",
    "                print(\"Top Scores:\", topk_logits)  \n",
    "\n",
    "                scores = scores.cpu().detach().numpy()  # Objectness scores in [0, 1]\n",
    "                boxes = pred_boxes[i].cpu().detach().numpy()  # Bounding boxes in unit scale\n",
    "\n",
    "\n",
    "                # Draw predictions on the image\n",
    "                image_with_boxes = draw_predictions(image_np.copy(), boxes, scores)\n",
    "\n",
    "                # Save the image with bounding boxes\n",
    "                output_file = f\"{output_image_path}/epoch_{epoch}_batch_{batch_idx}_img_{i}.jpg\"\n",
    "                cv2.imwrite(output_file, image_with_boxes)\n",
    "                print(f\"Saved image: {output_file}\")\n",
    "                \n",
    "\n",
    "\n",
    "        # Log the training loss\n",
    "        wandb.log({f\"train_{key}\": value for key, value in loss_dict.items()})\n",
    "\n",
    "        # Accumulate loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate the percentage of samples processed\n",
    "        percent_complete = 100 * (samples_processed / total_samples)\n",
    "\n",
    "        # Print training progress\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Iteration [{batch_idx+1}/{total_batches}] \"\n",
    "            f\"Loss: {loss.item():.4f} - Progress: {percent_complete:.2f}%\")\n",
    "        \n",
    "    avg_train_loss = running_loss / total_batches\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] completed. Average Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Logging to wandb\n",
    "    wandb.log({\"epoch\": epoch + 1, \"train_loss\": avg_train_loss})\n",
    "\n",
    "    # Save the current checkpoint\n",
    "    if scaler is not None:\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': detr.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scaler_state_dict': scaler.state_dict(),\n",
    "            'train_loss': avg_train_loss,\n",
    "        }, checkpoint_path)\n",
    "    else:\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': detr.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': avg_train_loss,\n",
    "        }, checkpoint_path)\n",
    "\n",
    "\n",
    "print('Finished Training')\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
